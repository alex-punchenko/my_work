{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11886200,"sourceType":"datasetVersion","datasetId":7470687}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install grad-cam","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-07-25T12:39:32.204336Z","iopub.execute_input":"2025-07-25T12:39:32.204868Z","iopub.status.idle":"2025-07-25T12:39:35.429840Z","shell.execute_reply.started":"2025-07-25T12:39:32.204839Z","shell.execute_reply":"2025-07-25T12:39:35.429016Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageDraw\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nimport cv2\nimport albumentations as A\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.models import resnet18, ResNet18_Weights\n\nfrom pytorch_grad_cam import GradCAM\nfrom pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-25T12:39:35.431269Z","iopub.execute_input":"2025-07-25T12:39:35.431515Z","iopub.status.idle":"2025-07-25T12:39:40.187991Z","shell.execute_reply.started":"2025-07-25T12:39:35.431492Z","shell.execute_reply":"2025-07-25T12:39:40.187213Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data preparation and analysis","metadata":{}},{"cell_type":"markdown","source":"**Load data**","metadata":{}},{"cell_type":"code","source":"train_path = '/kaggle/input/brain-ct-medical-imaging-colorized-dataset/Computed Tomography (CT) of the Brain/dataset/train'\ntest_path = '/kaggle/input/brain-ct-medical-imaging-colorized-dataset/Computed Tomography (CT) of the Brain/dataset/test'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T12:39:40.188764Z","iopub.execute_input":"2025-07-25T12:39:40.189097Z","iopub.status.idle":"2025-07-25T12:39:40.192741Z","shell.execute_reply.started":"2025-07-25T12:39:40.189079Z","shell.execute_reply":"2025-07-25T12:39:40.191766Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Function сreates a labeled DataFrame that maps image file paths to their corresponding class labels.**","metadata":{}},{"cell_type":"code","source":"def get_df(path):\n    \n    classes = os.listdir(path)\n    data = []\n\n    for cls in classes:\n        class_folder = os.path.join(path, cls)\n\n        if not os.path.isdir(class_folder):\n            continue\n\n        for fname in os.listdir(class_folder):\n            if fname.lower().endswith('.jpg'):\n                img_path = os.path.join(class_folder, fname)\n                data.append({\n                    'img_path': img_path,\n                    'target': cls                    \n                })\n\n    return pd.DataFrame(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T12:39:40.194476Z","iopub.execute_input":"2025-07-25T12:39:40.194813Z","iopub.status.idle":"2025-07-25T12:39:40.212669Z","shell.execute_reply.started":"2025-07-25T12:39:40.194795Z","shell.execute_reply":"2025-07-25T12:39:40.212081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train, test = get_df(train_path), get_df(test_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T12:39:40.213273Z","iopub.execute_input":"2025-07-25T12:39:40.213544Z","iopub.status.idle":"2025-07-25T12:39:40.244585Z","shell.execute_reply.started":"2025-07-25T12:39:40.213523Z","shell.execute_reply":"2025-07-25T12:39:40.244113Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_names = ['aneurysm', 'cancer', 'tumor']\nclass_to_idx = {cls_name: idx for idx, cls_name in enumerate(class_names)}\n#{'aneurysm': 0, 'cancer': 1, 'tumor': 2}\n\ntrain['target_encoded'] = train['target'].map(class_to_idx)\ntest['target_encoded'] = test['target'].map(class_to_idx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T12:39:40.245234Z","iopub.execute_input":"2025-07-25T12:39:40.245434Z","iopub.status.idle":"2025-07-25T12:39:40.252121Z","shell.execute_reply.started":"2025-07-25T12:39:40.245418Z","shell.execute_reply":"2025-07-25T12:39:40.251538Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Class distributions**","metadata":{}},{"cell_type":"code","source":"print('Class distributions:')\nprint(train['target'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T12:39:40.252787Z","iopub.execute_input":"2025-07-25T12:39:40.252999Z","iopub.status.idle":"2025-07-25T12:39:40.267856Z","shell.execute_reply.started":"2025-07-25T12:39:40.252977Z","shell.execute_reply":"2025-07-25T12:39:40.267114Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The data is evenly distributed. It's good!.","metadata":{}},{"cell_type":"markdown","source":"**Data visualizations**","metadata":{}},{"cell_type":"markdown","source":"For example:\nFirst image in train","metadata":{}},{"cell_type":"code","source":"def show_image(image_input):\n    \"\"\"\n    Universal image display function.\n    Accepts either a path to an image (str) or a torch.Tensor.\n    \"\"\"\n    if isinstance(image_input, str):\n        image = cv2.imread(image_input)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    elif isinstance(image_input, torch.Tensor):          \n        image = image_input.detach().cpu().numpy()  # (3, H, W)\n        image = np.transpose(image, (1, 2, 0)) \n    else:\n        raise TypeError('Input must be a file path (str) or a torch.Tensor.')\n\n    plt.imshow(image)\n    plt.axis('off')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T12:39:40.268714Z","iopub.execute_input":"2025-07-25T12:39:40.268893Z","iopub.status.idle":"2025-07-25T12:39:40.282314Z","shell.execute_reply.started":"2025-07-25T12:39:40.268879Z","shell.execute_reply":"2025-07-25T12:39:40.281622Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_image(train.img_path.iloc[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T12:39:40.282982Z","iopub.execute_input":"2025-07-25T12:39:40.283218Z","iopub.status.idle":"2025-07-25T12:39:40.409581Z","shell.execute_reply.started":"2025-07-25T12:39:40.283192Z","shell.execute_reply":"2025-07-25T12:39:40.408955Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Image grid by class**","metadata":{}},{"cell_type":"code","source":"def plot_grid_by_class(df, num_images_per_class=3, random_state=777):\n    \"\"\"\n    Displays a grid of sample images from each class.\n    Each column corresponds to a class; each row is an image from that class.\n    \"\"\"\n    classes = sorted(df['target'].unique())\n    num_classes = len(classes)\n    \n    samples = [\n        df[df['target'] == cls].sample(num_images_per_class, random_state=random_state)\n        for cls in classes\n    ]\n    sample_df = pd.concat(samples, keys=classes).reset_index(drop=True)\n\n    plt.figure(figsize=(3 * num_classes, 3 * num_images_per_class))\n\n    for col_idx, cls in enumerate(classes):\n        class_samples = sample_df[sample_df['target'] == cls].reset_index(drop=True)\n        for row_idx in range(num_images_per_class):\n            i = row_idx * num_classes + col_idx \n            img_path = class_samples.loc[row_idx, 'img_path']\n            img = Image.open(img_path)\n\n            plt.subplot(num_images_per_class, num_classes, i + 1)\n            plt.imshow(img)\n            plt.title(f'Class: {cls}')\n            plt.axis('off')\n            \n    plt.suptitle('Image grid by class', fontsize=16)\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T12:39:40.411759Z","iopub.execute_input":"2025-07-25T12:39:40.411948Z","iopub.status.idle":"2025-07-25T12:39:40.417926Z","shell.execute_reply.started":"2025-07-25T12:39:40.411934Z","shell.execute_reply":"2025-07-25T12:39:40.417200Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_grid_by_class(train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T12:39:40.418626Z","iopub.execute_input":"2025-07-25T12:39:40.418859Z","iopub.status.idle":"2025-07-25T12:39:41.286375Z","shell.execute_reply.started":"2025-07-25T12:39:40.418845Z","shell.execute_reply":"2025-07-25T12:39:41.285706Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In many examples, the images show similar artifacts (as illustrated below). Additionally, the scans are not uniform — they were taken using different machines and with varying CT parameters.","metadata":{}},{"cell_type":"code","source":"img = Image.open(train.img_path.iloc[0])\n\ndraw1 = ImageDraw.Draw(img)\ndraw2 = ImageDraw.Draw(img)\n\nleft, top = 390, 240\nright, bottom =img.width - 30, img.height - 20\ndraw1.rectangle([left, top, right, bottom], outline='red', width=3)\n\nleft, top = 10, 240\nright, bottom =img.width - 460, img.height - 120\ndraw1.rectangle([left, top, right, bottom], outline='red', width=3)\n\n\nplt.title('Example of artifacts ')\nplt.imshow(img)\nplt.axis('off')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T12:39:41.287272Z","iopub.execute_input":"2025-07-25T12:39:41.287542Z","iopub.status.idle":"2025-07-25T12:39:41.419213Z","shell.execute_reply.started":"2025-07-25T12:39:41.287521Z","shell.execute_reply":"2025-07-25T12:39:41.418464Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Data preprocessing**","metadata":{}},{"cell_type":"markdown","source":"Train-val split","metadata":{}},{"cell_type":"code","source":"train_df, val_df = train_test_split(\n    train, \n    test_size=0.15, \n    stratify=train['target'],  \n    random_state=777\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T12:39:41.420005Z","iopub.execute_input":"2025-07-25T12:39:41.420215Z","iopub.status.idle":"2025-07-25T12:39:41.427466Z","shell.execute_reply.started":"2025-07-25T12:39:41.420199Z","shell.execute_reply":"2025-07-25T12:39:41.426955Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Train_df shape: {train_df.shape[0]}\")\nprint(f\"Val_df shape: {val_df.shape[0]}\")\nprint(f\"test shape: {test.shape[0]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T12:39:41.428182Z","iopub.execute_input":"2025-07-25T12:39:41.428438Z","iopub.status.idle":"2025-07-25T12:39:41.440541Z","shell.execute_reply.started":"2025-07-25T12:39:41.428417Z","shell.execute_reply":"2025-07-25T12:39:41.439808Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"At this stage we have 3 data split:\n1. train_df 85%-train\n2. val_df 15%-train\n3. test 144 images","metadata":{}},{"cell_type":"markdown","source":"**Preprocess function**\n* Load img\n* Convert BGR to RGB\n* Resize\n* Augmentation (rotate, flip, contrast change)\n* Normalize pixels","metadata":{}},{"cell_type":"markdown","source":"Since the images are diverse in nature, augmentation will provide the model with greater variability during training. This will help during testing if the images are slightly rotated, have different contrast or brightness, or are flipped.","metadata":{}},{"cell_type":"code","source":"def preprocess_image(img_path, size=224, augment=False):\n    image = cv2.imread(img_path)\n    \n    if image is None:\n        raise ValueError(f'Image not found at path: {img_path}')\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Convert BGR to RGB\n    image = cv2.resize(image, (size, size)) #Resize to (size x size)\n    # Apply augmentation if requested\n    if augment:\n        transform = A.Compose([\n            A.Rotate(limit=15, p=1.0),\n            A.HorizontalFlip(p=0.5),\n            A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),\n        ])\n        augmented = transform(image=image)\n        image = augmented['image']\n\n    image = image.astype(np.float32) / 255.0 \n\n    # ImageNet mean/std\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    image = (image - mean) / std\n\n    # transpose in (C,H,W)\n    image = np.transpose(image, (2, 0, 1))\n\n    return torch.tensor(image, dtype=torch.float32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T12:39:41.456267Z","iopub.execute_input":"2025-07-25T12:39:41.456607Z","iopub.status.idle":"2025-07-25T12:39:41.471674Z","shell.execute_reply.started":"2025-07-25T12:39:41.456586Z","shell.execute_reply":"2025-07-25T12:39:41.471016Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Examples: \n1. Image before processing\n2. Image after processing without augmentation\n3. Image after processing with augmentation\n ","metadata":{}},{"cell_type":"code","source":"plt.title('Image before processing')\nshow_image(train_df.img_path.iloc[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T12:39:41.472399Z","iopub.execute_input":"2025-07-25T12:39:41.472696Z","iopub.status.idle":"2025-07-25T12:39:41.614988Z","shell.execute_reply.started":"2025-07-25T12:39:41.472672Z","shell.execute_reply":"2025-07-25T12:39:41.614192Z"},"_kg_hide-input":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.title('Image after processing without augmentation')\nshow_image(preprocess_image(train_df.img_path.iloc[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T12:39:41.615853Z","iopub.execute_input":"2025-07-25T12:39:41.616089Z","iopub.status.idle":"2025-07-25T12:39:41.769304Z","shell.execute_reply.started":"2025-07-25T12:39:41.616074Z","shell.execute_reply":"2025-07-25T12:39:41.768531Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.title('Image after processing with augmentation')\nshow_image(preprocess_image(train_df.img_path.iloc[0],augment=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T12:39:41.770244Z","iopub.execute_input":"2025-07-25T12:39:41.770515Z","iopub.status.idle":"2025-07-25T12:39:41.930316Z","shell.execute_reply.started":"2025-07-25T12:39:41.770493Z","shell.execute_reply":"2025-07-25T12:39:41.928592Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model building and training","metadata":{}},{"cell_type":"code","source":"class CustomImageDataset(Dataset):\n    def __init__(self, df, augment=False):\n        \"\"\"\n        Args:\n            df (pd.DataFrame): DataFrame with 'img_path' and 'target' columns\n            augment (bool): Whether to apply augmentation\n        \"\"\"\n        self.img_paths = df['img_path'].values\n        self.labels = df['target_encoded'].values\n        self.augment = augment\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.img_paths[idx]\n        label = self.labels[idx]\n        image = preprocess_image(img_path, augment=self.augment)\n        return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T12:39:41.937820Z","iopub.execute_input":"2025-07-25T12:39:41.938142Z","iopub.status.idle":"2025-07-25T12:39:41.949808Z","shell.execute_reply.started":"2025-07-25T12:39:41.938117Z","shell.execute_reply":"2025-07-25T12:39:41.948979Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split into train/val beforehand → train_df, val_df\n\ntrain_dataset = CustomImageDataset(train_df, augment=True)\nval_dataset   = CustomImageDataset(val_df, augment=False)\ntest_dataset = CustomImageDataset(test, augment = False)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2, pin_memory=True)\nval_loader   = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2, pin_memory=True)\ntest_loader   = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T12:39:41.950587Z","iopub.execute_input":"2025-07-25T12:39:41.950862Z","iopub.status.idle":"2025-07-25T12:39:41.964285Z","shell.execute_reply.started":"2025-07-25T12:39:41.950840Z","shell.execute_reply":"2025-07-25T12:39:41.963422Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Bilding model. Add Dropout 50% + output 3 layers\n\nclass ResNet18Custom(nn.Module):\n    def __init__(self, dropout_p=0.5, weights=ResNet18_Weights.DEFAULT, num_classes=3):\n        super(ResNet18Custom, self).__init__()\n        self.base_model = resnet18(weights=weights)\n        in_features = self.base_model.fc.in_features\n        self.base_model.fc = nn.Sequential(\n            nn.Dropout(dropout_p),\n            nn.Linear(in_features, num_classes)\n        )\n\n    def forward(self, x):\n        return self.base_model(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T12:39:41.965039Z","iopub.execute_input":"2025-07-25T12:39:41.965301Z","iopub.status.idle":"2025-07-25T12:39:41.983305Z","shell.execute_reply.started":"2025-07-25T12:39:41.965279Z","shell.execute_reply":"2025-07-25T12:39:41.982772Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# EarlyStopping Class\n\nclass EarlyStopping:\n    def __init__(self, patience=5, verbose=True, delta=0):\n        self.patience = patience\n        self.verbose = verbose\n        self.delta = delta\n        self.best_loss = None\n        self.counter = 0\n        self.early_stop = False\n\n    def __call__(self, val_loss):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n            if self.verbose:\n                print(f'Initial val loss: {val_loss:.4f}')\n        elif val_loss > self.best_loss - self.delta:\n            self.counter += 1\n            if self.verbose:\n                print(f'No improvement in val loss for {self.counter} epoch(s)')\n            if self.counter >= self.patience:\n                if self.verbose:\n                    print(f'Early stopping triggered after {self.patience} epochs without improvement.')\n                self.early_stop = True\n        else:\n            if self.verbose:\n                print(f'Validation loss improved from {self.best_loss:.4f} to {val_loss:.4f}')\n            self.best_loss = val_loss\n            self.counter = 0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T12:39:41.983985Z","iopub.execute_input":"2025-07-25T12:39:41.984204Z","iopub.status.idle":"2025-07-25T12:39:41.998806Z","shell.execute_reply.started":"2025-07-25T12:39:41.984181Z","shell.execute_reply":"2025-07-25T12:39:41.998102Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = ResNet18Custom(dropout_p=0.5, weights=ResNet18_Weights.DEFAULT).to(device)\nmodel = torch.nn.DataParallel(model)  #Parallel train on 2 device T4\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\nearly_stopping = EarlyStopping(patience=5, verbose=True) #EarlyStopping 5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T12:39:41.999497Z","iopub.execute_input":"2025-07-25T12:39:42.000156Z","iopub.status.idle":"2025-07-25T12:39:42.438799Z","shell.execute_reply.started":"2025-07-25T12:39:42.000138Z","shell.execute_reply":"2025-07-25T12:39:42.438178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_losses = []\nval_losses = []\ntrain_accuracies = []\nval_accuracies = []\n\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0\n    all_train_preds = []\n    all_train_targets = []\n    total = 0\n\n    # Training\n    for inputs, targets in train_loader:\n        inputs, targets = inputs.to(device), targets.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item() * inputs.size(0)\n        total += inputs.size(0)\n\n        preds = torch.argmax(outputs, dim=1)\n        all_train_preds.extend(preds.cpu().numpy())\n        all_train_targets.extend(targets.cpu().numpy())\n\n    train_loss /= total\n    train_acc = accuracy_score(all_train_targets, all_train_preds)\n\n    # Validation\n    model.eval()\n    val_loss = 0\n    all_val_preds = []\n    all_val_targets = []\n    total_val = 0\n\n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            val_loss += loss.item() * inputs.size(0)\n            total_val += inputs.size(0)\n\n            preds = torch.argmax(outputs, dim=1)\n            all_val_preds.extend(preds.cpu().numpy())\n            all_val_targets.extend(targets.cpu().numpy())\n\n    val_loss /= total_val\n    val_acc = accuracy_score(all_val_targets, all_val_preds)\n\n    train_losses.append(train_loss)\n    val_losses.append(val_loss)\n    train_accuracies.append(train_acc)\n    val_accuracies.append(val_acc)\n\n    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n\n    early_stopping(val_loss)\n    if early_stopping.early_stop:\n        print('Stopping training early')\n        break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T12:39:42.439483Z","iopub.execute_input":"2025-07-25T12:39:42.439755Z","iopub.status.idle":"2025-07-25T12:41:36.959572Z","shell.execute_reply.started":"2025-07-25T12:39:42.439732Z","shell.execute_reply":"2025-07-25T12:41:36.958789Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test metrics\nmodel.eval()\nall_preds = []\nall_targets = []\n\nwith torch.no_grad():\n    for inputs, targets in test_loader:\n        inputs, targets = inputs.to(device), targets.to(device)\n        outputs = model(inputs)\n        preds = torch.argmax(outputs, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_targets.extend(targets.cpu().numpy())\n\ntest_acc = accuracy_score(all_targets, all_preds)\nprint(f'Test Accuracy: {test_acc:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T14:03:29.243347Z","iopub.execute_input":"2025-07-25T14:03:29.244110Z","iopub.status.idle":"2025-07-25T14:03:30.172511Z","shell.execute_reply.started":"2025-07-25T14:03:29.244077Z","shell.execute_reply":"2025-07-25T14:03:30.171484Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(confusion_matrix(all_targets, all_preds)) # Сonfusion_matrix\nprint(classification_report(all_targets, all_preds, target_names=class_names))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T12:41:48.865171Z","iopub.execute_input":"2025-07-25T12:41:48.865458Z","iopub.status.idle":"2025-07-25T12:41:48.878353Z","shell.execute_reply.started":"2025-07-25T12:41:48.865433Z","shell.execute_reply":"2025-07-25T12:41:48.877543Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Wow! Zero errors!","metadata":{}},{"cell_type":"markdown","source":"**Loss and accuracy plots**","metadata":{}},{"cell_type":"code","source":"epochs = range(1, len(train_losses) + 1)\n\nplt.figure(figsize=(12, 5))\n\n# Loss\nplt.subplot(1, 2, 1)\nplt.plot(epochs, train_losses, label='Train Loss')\nplt.plot(epochs, val_losses, label='Val Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss over Epochs')\nplt.legend()\n\n# Accuracy\nplt.subplot(1, 2, 2)\nplt.plot(epochs, train_accuracies, label='Train Accuracy')\nplt.plot(epochs, val_accuracies, label='Val Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Accuracy over Epochs')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T14:04:06.244873Z","iopub.execute_input":"2025-07-25T14:04:06.245192Z","iopub.status.idle":"2025-07-25T14:04:06.629595Z","shell.execute_reply.started":"2025-07-25T14:04:06.245165Z","shell.execute_reply":"2025-07-25T14:04:06.628924Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"the plots show that up to the 2nd epoch there is a sharp improvement in metrics, then the metrics remain around the same level.","metadata":{}},{"cell_type":"markdown","source":"## Analysis of results","metadata":{}},{"cell_type":"markdown","source":"Heatmap (Grad-CAM) for key cases","metadata":{}},{"cell_type":"code","source":"img_path = val_df.sample(1).iloc[0]['img_path']\n# Prepare input tensor\ninput_tensor = preprocess_image(img_path).unsqueeze(0).to(device)\n\n# Prepare original image for overlay\nimg_np = cv2.imread(img_path)\nimg_np = cv2.cvtColor(img_np, cv2.COLOR_BGR2RGB)\nimg_np = cv2.resize(img_np, (224, 224))\nimg_np = img_np.astype(np.float32) / 255.0\n\n# Choose target layer (last conv block of resnet18)\ntarget_layer = model.module.base_model.layer4[-1]\n\n# Create GradCAM object\ncam = GradCAM(model=model, target_layers=[target_layer])\n\nmodel.eval()\nwith torch.no_grad():\n    output = model(input_tensor)\n    pred_class = output.argmax(dim=1).item()\n\n# Get heatmap\ngrayscale_cam = cam(input_tensor=input_tensor, targets=[ClassifierOutputTarget(pred_class)])\ngrayscale_cam = grayscale_cam[0]\n\n# Overlay heatmap\nvisualization = show_cam_on_image(img_np, grayscale_cam, use_rgb=True)\n\n# Show result\nplt.figure(figsize=(6, 6))\nplt.imshow(visualization)\nplt.title(f'Predicted class: {pred_class}')\nplt.axis('off')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T14:04:09.485831Z","iopub.execute_input":"2025-07-25T14:04:09.486135Z","iopub.status.idle":"2025-07-25T14:04:09.710363Z","shell.execute_reply.started":"2025-07-25T14:04:09.486114Z","shell.execute_reply":"2025-07-25T14:04:09.709462Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Visualize gradcam grid 3*3 ","metadata":{}},{"cell_type":"code","source":"def visualize_gradcam_grid(model, df, device):\n    \"\"\"\n    display a 3x3 grid of Grad-CAM visualizations.\n\n    for each of the three target classes (0, 1, 2), the function randomly selects\n    three images from the provided DataFrame and visualizes the Grad-CAM heatmap\n    for the specified class.\n    \n    \"\"\"\n    model.eval()\n    fig, axs = plt.subplots(3, 3, figsize=(15, 15))\n\n    for class_idx in range(3):\n        samples = df[df['target_encoded'] == class_idx].sample(3).reset_index(drop=True)\n        for i in range(3):\n            img_path = samples.loc[i, 'img_path']\n\n            # Preprocess\n            input_tensor = preprocess_image(img_path).unsqueeze(0).to(device)\n\n            img_np = cv2.imread(img_path)\n            img_np = cv2.cvtColor(img_np, cv2.COLOR_BGR2RGB)\n            img_np = cv2.resize(img_np, (224, 224))\n            img_np = img_np.astype(np.float32) / 255.0\n\n            target_layer = model.module.base_model.layer4[-1]\n            cam = GradCAM(model=model, target_layers=[target_layer])\n\n            grayscale_cam = cam(input_tensor=input_tensor, targets=[ClassifierOutputTarget(class_idx)])\n            grayscale_cam = grayscale_cam[0]\n\n            visualization = show_cam_on_image(img_np, grayscale_cam, use_rgb=True)\n\n            ax = axs[class_idx, i]\n            ax.imshow(visualization)\n            ax.set_title(f'Class {class_idx}')\n            ax.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T14:04:14.863129Z","iopub.execute_input":"2025-07-25T14:04:14.863411Z","iopub.status.idle":"2025-07-25T14:04:14.870651Z","shell.execute_reply.started":"2025-07-25T14:04:14.863390Z","shell.execute_reply":"2025-07-25T14:04:14.869835Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"visualize_gradcam_grid(model, test, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T14:04:17.583760Z","iopub.execute_input":"2025-07-25T14:04:17.584026Z","iopub.status.idle":"2025-07-25T14:04:19.196983Z","shell.execute_reply.started":"2025-07-25T14:04:17.584007Z","shell.execute_reply":"2025-07-25T14:04:19.195987Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Conclusion\n\nIf the model achieves 100% accuracy on the test set, it is worth questioning its practical applicability.\n\nAs seen in the heatmap:\nFor class 0, the model’s attention is focused on specific artifacts typical for this class (see example above).\nFor classes 1 and 2, the diagnostic relevance is slightly better — the attention shifts toward the posterior part of the image and varies at times.\n\nSuggestions for improvement:\n\n1. Increase the size of the test set\n\n2. Group images by patient\n\n3. Reduce image artifacts\n\n4. Standardize the images","metadata":{}}]}